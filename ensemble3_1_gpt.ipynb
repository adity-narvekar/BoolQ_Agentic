{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2691c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble on ensemble3_train.csv 3-2/2-3 splits...\n",
      "Evaluated Ensemble\n",
      "Saved ensemble3_1_train.csv\n",
      "Ensemble Accuracy: 0.66\n",
      "Saved ensemble3_1_train.json\n",
      "Evaluating Ensemble on ensemble3_valid.csv 3-2/2-3 splits...\n",
      "Evaluated Ensemble\n",
      "Saved ensemble3_1_valid.csv\n",
      "Ensemble Accuracy: 0.69\n",
      "Saved ensemble3_1_valid.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import string\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "load_dotenv(\"env.env\")\n",
    "\n",
    "client_phi = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"PHI_AZURE_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"PHI_AZURE_KEY\"))\n",
    ")\n",
    "client_mistral = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"MISTRAL_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"MISTRAL_KEY\"))\n",
    ")\n",
    "client_llama = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"LLAMA_3_8B_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"LLAMA_3_8B_KEY\"))\n",
    ")\n",
    "llm4o = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "llm41 = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"GPT_4_1_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"GPT_4_1_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "def get_phi_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_phi.complete(messages=messages, model=\"phi\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_mistral_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_mistral.complete(messages=messages, model=\"mistral-nemo\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_llama_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_llama.complete(messages=messages, model=\"llama-8b\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_4o_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm4o.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_41_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm41.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def parse_answer(answer_text):\n",
    "    lines = answer_text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        cleaned = line.strip().strip(string.punctuation).lower()\n",
    "        if cleaned in ['yes', 'no']:\n",
    "            return cleaned == 'yes'\n",
    "    return None\n",
    "\n",
    "def is_3_2_split(row):\n",
    "    preds = [row['phi_pred'], row['mistral_pred'], row['llama_pred'], row['gpt4o_mini_pred'], row['gpt41_mini_pred']]\n",
    "    yes_count = sum([p is True for p in preds])\n",
    "    no_count = sum([p is False for p in preds])\n",
    "    return (yes_count == 3 and no_count == 2) or (yes_count == 2 and no_count == 3)\n",
    "\n",
    "def get_ensemble_prediction(passage, question, true_answer):\n",
    "    prompt = f\"Passage: {passage}\\nQuestion: {question}\\nBased on the provided passage, answer the provided question carefully with just yes or no.\"\n",
    "    model_funcs = [\n",
    "        (\"phi\", get_phi_completion),\n",
    "        (\"mistral\", get_mistral_completion),\n",
    "        (\"llama\", get_llama_completion),\n",
    "        (\"gpt4o_mini\", get_4o_mini_completion),\n",
    "        (\"gpt41_mini\", get_41_mini_completion)\n",
    "    ]\n",
    "    preds = []\n",
    "    model_preds = {}\n",
    "    for name, func in model_funcs:\n",
    "        try:\n",
    "            answer = func(prompt)\n",
    "            parsed = parse_answer(answer)\n",
    "            model_preds[f\"{name}_raw\"] = answer\n",
    "            model_preds[f\"{name}_pred\"] = parsed\n",
    "            if parsed is not None:\n",
    "                preds.append(parsed)\n",
    "        except Exception:\n",
    "            model_preds[f\"{name}_raw\"] = None\n",
    "            model_preds[f\"{name}_pred\"] = None\n",
    "    if len(preds) == 0:\n",
    "        return None, False, model_preds\n",
    "    vote = sum(preds) > len(preds) // 2\n",
    "    return vote, vote == true_answer, model_preds\n",
    "\n",
    "def run_ensemble_on_3_2(csv_in, csv_out, json_out):\n",
    "    df = pd.read_csv(csv_in)\n",
    "    df = df[df.apply(is_3_2_split, axis=1)]\n",
    "    flagged_indices = set()\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Evaluating Ensemble on {csv_in} 3-2/2-3 splits...\")\n",
    "    for i, row in df.iterrows():\n",
    "        passage = row[\"passage\"]\n",
    "        question = row[\"question\"]\n",
    "        true_answer = row[\"actual_answer\"]\n",
    "        pred, is_correct, model_preds = get_ensemble_prediction(passage, question, true_answer)\n",
    "        if pred is None:\n",
    "            flagged_indices.add(i)\n",
    "            continue\n",
    "        entry = {\n",
    "            \"q_number\": row[\"q_number\"],\n",
    "            \"passage\": passage,\n",
    "            \"question\": question,\n",
    "            \"actual_answer\": true_answer,\n",
    "            \"ensemble_pred\": pred,\n",
    "            \"phi_pred\": model_preds[\"phi_pred\"],\n",
    "            \"mistral_pred\": model_preds[\"mistral_pred\"],\n",
    "            \"llama_pred\": model_preds[\"llama_pred\"],\n",
    "            \"gpt4o_mini_pred\": model_preds[\"gpt4o_mini_pred\"],\n",
    "            \"gpt41_mini_pred\": model_preds[\"gpt41_mini_pred\"]\n",
    "        }\n",
    "        results.append(entry)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "    acc_ensemble = correct / total if total > 0 else 0\n",
    "    print(\"Evaluated Ensemble\")\n",
    "    df_ensemble = pd.DataFrame(results)\n",
    "    df_ensemble.to_csv(csv_out, index=False)\n",
    "    print(f\"Saved {csv_out}\")\n",
    "    print(f\"Ensemble Accuracy: {acc_ensemble:.2f}\")\n",
    "    final_output = {\n",
    "        \"ensemble_accuracy\": acc_ensemble,\n",
    "        \"correct_count\": correct,\n",
    "        \"flagged_indices\": list(flagged_indices)\n",
    "    }\n",
    "    with open(json_out, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "    print(f\"Saved {json_out}\")\n",
    "\n",
    "run_ensemble_on_3_2(\"ensemble3_train.csv\", \"ensemble3_1_train.csv\", \"ensemble3_1_train.json\")\n",
    "run_ensemble_on_3_2(\"ensemble3_valid.csv\", \"ensemble3_1_valid.csv\", \"ensemble3_1_valid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a72d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GPT-4o on ensemble3_1_train.csv (only 3-2/2-3 splits)...\n",
      "Evaluated\n",
      "Saved ensemble3_gpt_train.csv\n",
      "GPT-4o Accuracy: 0.70\n",
      "Saved ensemble3_gpt_train.json\n",
      "Evaluating GPT-4o on ensemble3_1_valid.csv (only 3-2/2-3 splits)...\n",
      "Evaluated\n",
      "Saved ensemble3_gpt_valid.csv\n",
      "GPT-4o Accuracy: 0.73\n",
      "Saved ensemble3_gpt_valid.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import string\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv(\"env.env\")\n",
    "\n",
    "llm4o = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "def get_4o_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm4o.chat.completions.create(\n",
    "        model=\"gpt4o\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def parse_answer(answer_text):\n",
    "    lines = answer_text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        cleaned = line.strip().strip(string.punctuation).lower()\n",
    "        if cleaned in ['yes', 'no']:\n",
    "            return cleaned == 'yes'\n",
    "    return None\n",
    "\n",
    "def is_3_2_split(row):\n",
    "    preds = [row['phi_pred'], row['mistral_pred'], row['llama_pred'], row['gpt4o_mini_pred'], row['gpt41_mini_pred']]\n",
    "    yes_count = sum([p is True for p in preds])\n",
    "    no_count = sum([p is False for p in preds])\n",
    "    return (yes_count == 3 and no_count == 2) or (yes_count == 2 and no_count == 3)\n",
    "\n",
    "def evaluate_model_on_ensemble3_1(csv_in, csv_out, json_out):\n",
    "    df = pd.read_csv(csv_in)\n",
    "    # Only keep 3-2/2-3 splits\n",
    "    df = df[df.apply(is_3_2_split, axis=1)]\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Evaluating GPT-4o on {csv_in} (only 3-2/2-3 splits)...\")\n",
    "    for i, row in df.iterrows():\n",
    "        passage = row[\"passage\"]\n",
    "        question = row[\"question\"]\n",
    "        true_answer = row[\"actual_answer\"]\n",
    "        prompt = f\"Passage: {passage}\\nQuestion: {question}\\nAnswer with yes or no.\"\n",
    "        try:\n",
    "            answer_text = get_4o_completion(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping sample {i} due to error: {e}\")\n",
    "            continue\n",
    "        pred = parse_answer(answer_text)\n",
    "        is_correct = (pred == true_answer)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        results.append({\n",
    "            \"q_number\": row[\"q_number\"],\n",
    "            \"passage\": passage,\n",
    "            \"question\": question,\n",
    "            \"actual_answer\": true_answer,\n",
    "            \"4o_pred\": pred\n",
    "        })\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(\"Evaluated\")\n",
    "    df_out = pd.DataFrame(results)\n",
    "    df_out.to_csv(csv_out, index=False)\n",
    "    print(f\"Saved {csv_out}\")\n",
    "    print(f\"GPT-4o Accuracy: {accuracy:.2f}\")\n",
    "    final_output = {\n",
    "        \"model_accuracy\": accuracy,\n",
    "        \"correct_count\": correct\n",
    "    }\n",
    "    with open(json_out, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=2)\n",
    "    print(f\"Saved {json_out}\")\n",
    "\n",
    "evaluate_model_on_ensemble3_1(\"ensemble3_1_train.csv\", \"ensemble3_gpt_train.csv\", \"ensemble3_gpt_train.json\")\n",
    "evaluate_model_on_ensemble3_1(\"ensemble3_1_valid.csv\", \"ensemble3_gpt_valid.csv\", \"ensemble3_gpt_valid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0355f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ensemble3_vs_3_1_train.json\n",
      "Saved ensemble3_vs_3_1_valid.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def is_3_2_split(row):\n",
    "    preds = [row['phi_pred'], row['mistral_pred'], row['llama_pred'], row['gpt4o_mini_pred'], row['gpt41_mini_pred']]\n",
    "    yes_count = sum([p is True for p in preds])\n",
    "    no_count = sum([p is False for p in preds])\n",
    "    return (yes_count == 3 and no_count == 2) or (yes_count == 2 and no_count == 3)\n",
    "\n",
    "def compare_ensembles(csv_a, csv_b, out_json):\n",
    "    df_a = pd.read_csv(csv_a)\n",
    "    df_b = pd.read_csv(csv_b)\n",
    "    df_a = df_a[df_a.apply(is_3_2_split, axis=1)]\n",
    "    df_a = df_a.set_index(\"q_number\")\n",
    "    df_b = df_b.set_index(\"q_number\")\n",
    "    common = df_a.index.intersection(df_b.index)\n",
    "    total = len(common)\n",
    "    correct_a = 0\n",
    "    correct_b = 0\n",
    "    correct_a_not_b = 0\n",
    "    correct_b_not_a = 0\n",
    "    for qn in common:\n",
    "        actual = df_a.loc[qn, \"actual_answer\"]\n",
    "        pred_a = df_a.loc[qn, \"ensemble_pred\"]\n",
    "        pred_b = df_b.loc[qn, \"ensemble_pred\"]\n",
    "        is_a = (pred_a == actual)\n",
    "        is_b = (pred_b == actual)\n",
    "        if is_a:\n",
    "            correct_a += 1\n",
    "        if is_b:\n",
    "            correct_b += 1\n",
    "        if is_a and not is_b:\n",
    "            correct_a_not_b += 1\n",
    "        if is_b and not is_a:\n",
    "            correct_b_not_a += 1\n",
    "    out = {\n",
    "        \"total_questions\": total,\n",
    "        \"correct_ensemble3\": correct_a,\n",
    "        \"correct_ensemble3_1\": correct_b,\n",
    "        \"correct_ensemble3_not_ensemble3_1\": correct_a_not_b,\n",
    "        \"correct_ensemble3_1_not_ensemble3\": correct_b_not_a\n",
    "    }\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved {out_json}\")\n",
    "\n",
    "compare_ensembles(\"ensemble3_train.csv\", \"ensemble3_1_train.csv\", \"ensemble3_vs_3_1_train.json\")\n",
    "compare_ensembles(\"ensemble3_valid.csv\", \"ensemble3_1_valid.csv\", \"ensemble3_vs_3_1_valid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbc3928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ensemble3_1_vs_gpt_train.json\n",
      "Saved ensemble3_1_vs_gpt_valid.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def is_3_2_split(row):\n",
    "    preds = [row['phi_pred'], row['mistral_pred'], row['llama_pred'], row['gpt4o_mini_pred'], row['gpt41_mini_pred']]\n",
    "    yes_count = sum([p is True for p in preds])\n",
    "    no_count = sum([p is False for p in preds])\n",
    "    return (yes_count == 3 and no_count == 2) or (yes_count == 2 and no_count == 3)\n",
    "\n",
    "def compare_ensemble_and_gpt(csv_ens, csv_gpt, out_json):\n",
    "    df_ens = pd.read_csv(csv_ens)\n",
    "    df_gpt = pd.read_csv(csv_gpt)\n",
    "    df_ens = df_ens[df_ens.apply(is_3_2_split, axis=1)]\n",
    "    df_ens = df_ens.set_index(\"q_number\")\n",
    "    df_gpt = df_gpt.set_index(\"q_number\")\n",
    "    common = df_ens.index.intersection(df_gpt.index)\n",
    "    total = len(common)\n",
    "    correct_ens = 0\n",
    "    correct_gpt = 0\n",
    "    correct_ens_not_gpt = 0\n",
    "    correct_gpt_not_ens = 0\n",
    "    for qn in common:\n",
    "        actual = df_ens.loc[qn, \"actual_answer\"]\n",
    "        pred_ens = df_ens.loc[qn, \"ensemble_pred\"]\n",
    "        pred_gpt = df_gpt.loc[qn, \"4o_pred\"]\n",
    "        is_ens = (pred_ens == actual)\n",
    "        is_gpt = (pred_gpt == actual)\n",
    "        if is_ens:\n",
    "            correct_ens += 1\n",
    "        if is_gpt:\n",
    "            correct_gpt += 1\n",
    "        if is_ens and not is_gpt:\n",
    "            correct_ens_not_gpt += 1\n",
    "        if is_gpt and not is_ens:\n",
    "            correct_gpt_not_ens += 1\n",
    "    out = {\n",
    "        \"total_questions\": total,\n",
    "        \"correct_ensemble\": correct_ens,\n",
    "        \"correct_gpt\": correct_gpt,\n",
    "        \"correct_ensemble_not_gpt\": correct_ens_not_gpt,\n",
    "        \"correct_gpt_not_ensemble\": correct_gpt_not_ens\n",
    "    }\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved {out_json}\")\n",
    "\n",
    "compare_ensemble_and_gpt(\"ensemble3_1_train.csv\", \"ensemble3_gpt_train.csv\", \"ensemble3_1_vs_gpt_train.json\")\n",
    "compare_ensemble_and_gpt(\"ensemble3_1_valid.csv\", \"ensemble3_gpt_valid.csv\", \"ensemble3_1_vs_gpt_valid.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
