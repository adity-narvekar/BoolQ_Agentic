{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akula\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble...\n",
      "Evaluated Ensemble\n",
      "Saved ensemble3_train.csv\n",
      "Ensemble Accuracy: 0.89\n",
      "ensemble3_train.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import string\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import UserMessage\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\"env.env\")\n",
    "\n",
    "client_phi = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"PHI_AZURE_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"PHI_AZURE_KEY\"))\n",
    ")\n",
    "\n",
    "client_mistral = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"MISTRAL_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"MISTRAL_KEY\"))\n",
    ")\n",
    "\n",
    "client_llama = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"LLAMA_3_8B_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"LLAMA_3_8B_KEY\"))\n",
    ")\n",
    "\n",
    "llm4o = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "llm41 = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"GPT_4_1_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"GPT_4_1_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "def get_phi_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_phi.complete(messages=messages, model=\"phi\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_mistral_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_mistral.complete(messages=messages, model=\"mistral-nemo\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_llama_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_llama.complete(messages=messages, model=\"llama-8b\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_4o_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm4o.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_41_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm41.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ds = load_dataset(\"boolq\", split={\"train\": \"train\", \"validation\": \"validation\"})\n",
    "# combined_dataset = concatenate_datasets([ds[\"train\"], ds[\"validation\"]])\n",
    "# dataset = combined_dataset\n",
    "\n",
    "dataset = load_dataset(\"boolq\", split=\"train\")\n",
    "\n",
    "def parse_answer(answer_text):\n",
    "    lines = answer_text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        cleaned = line.strip().strip(string.punctuation).lower()\n",
    "        if cleaned in ['yes', 'no']:\n",
    "            return cleaned == 'yes'\n",
    "    return None\n",
    "\n",
    "def get_ensemble_prediction(passage, question, true_answer):\n",
    "    prompt = f\"Passage: {passage}\\nQuestion: {question}\\nAnswer with yes or no.\"\n",
    "    model_funcs = [\n",
    "        (\"phi\", get_phi_completion),\n",
    "        (\"mistral\", get_mistral_completion),\n",
    "        (\"llama\", get_llama_completion),\n",
    "        (\"gpt4o_mini\", get_4o_mini_completion),\n",
    "        (\"gpt41_mini\", get_41_mini_completion)\n",
    "    ]\n",
    "    preds = []\n",
    "    model_preds = {}\n",
    "    for name, func in model_funcs:\n",
    "        try:\n",
    "            answer = func(prompt)\n",
    "            parsed = parse_answer(answer)\n",
    "            model_preds[f\"{name}_raw\"] = answer\n",
    "            model_preds[f\"{name}_pred\"] = parsed\n",
    "            if parsed is not None:\n",
    "                preds.append(parsed)\n",
    "        except Exception:\n",
    "            model_preds[f\"{name}_raw\"] = None\n",
    "            model_preds[f\"{name}_pred\"] = None\n",
    "    if len(preds) == 0:\n",
    "        return None, False, model_preds\n",
    "    vote = sum(preds) > len(preds) // 2\n",
    "    return vote, vote == true_answer, model_preds\n",
    "\n",
    "flagged_indices = set()\n",
    "results = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Evaluating Ensemble...\")\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i in flagged_indices:\n",
    "        continue\n",
    "\n",
    "    passage = example[\"passage\"]\n",
    "    question = example[\"question\"]\n",
    "    true_answer = example[\"answer\"]\n",
    "\n",
    "    pred, is_correct, model_preds = get_ensemble_prediction(passage, question, true_answer)\n",
    "\n",
    "    if pred is None:\n",
    "        flagged_indices.add(i)\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "        \"q_number\": i + 1,\n",
    "        \"passage\": passage,\n",
    "        \"question\": question,\n",
    "        \"actual_answer\": true_answer,\n",
    "        \"ensemble_pred\": pred,\n",
    "        \"phi_pred\": model_preds[\"phi_pred\"],\n",
    "        \"mistral_pred\": model_preds[\"mistral_pred\"],\n",
    "        \"llama_pred\": model_preds[\"llama_pred\"],\n",
    "        \"gpt4o_mini_pred\": model_preds[\"gpt4o_mini_pred\"],\n",
    "        \"gpt41_mini_pred\": model_preds[\"gpt41_mini_pred\"]\n",
    "    }\n",
    "    results.append(entry)\n",
    "\n",
    "    correct += int(is_correct)\n",
    "    total += 1\n",
    "\n",
    "acc_ensemble = correct / total if total > 0 else 0\n",
    "print(\"Evaluated Ensemble\")\n",
    "\n",
    "df_ensemble = pd.DataFrame(results)\n",
    "df_ensemble.to_csv(\"ensemble3_train.csv\", index=False)\n",
    "print(\"Saved ensemble3_train.csv\")\n",
    "print(f\"Ensemble Accuracy: {acc_ensemble:.2f}\")\n",
    "\n",
    "final_output = {\n",
    "    \"ensemble_accuracy\": acc_ensemble,\n",
    "    \"correct_count\": correct,\n",
    "    \"flagged_indices\": list(flagged_indices)\n",
    "}\n",
    "\n",
    "with open(\"ensemble3_train.json\", \"w\") as f:\n",
    "    json.dump(final_output, f, indent=2)\n",
    "print(\"ensemble3_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a72d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble...\n",
      "Evaluated Ensemble\n",
      "Saved ensemble3_valid.csv\n",
      "Ensemble Accuracy: 0.89\n",
      "Saved ensemble3_valid.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import string\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import UserMessage\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\"env.env\")\n",
    "\n",
    "client_phi = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"PHI_AZURE_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"PHI_AZURE_KEY\"))\n",
    ")\n",
    "\n",
    "client_mistral = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"MISTRAL_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"MISTRAL_KEY\"))\n",
    ")\n",
    "\n",
    "client_llama = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"LLAMA_3_8B_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"LLAMA_3_8B_KEY\"))\n",
    ")\n",
    "\n",
    "llm4o = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "llm41 = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"GPT_4_1_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"GPT_4_1_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "def get_phi_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_phi.complete(messages=messages, model=\"phi\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_mistral_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_mistral.complete(messages=messages, model=\"mistral-nemo\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_llama_completion(prompt, temperature=0.1):\n",
    "    messages = [UserMessage(content=prompt)]\n",
    "    response = client_llama.complete(messages=messages, model=\"llama-8b\", temperature=temperature)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_4o_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm4o.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_41_mini_completion(prompt, temperature=0.1):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = llm41.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ds = load_dataset(\"boolq\", split={\"train\": \"train\", \"validation\": \"validation\"})\n",
    "# combined_dataset = concatenate_datasets([ds[\"train\"], ds[\"validation\"]])\n",
    "# dataset = combined_dataset\n",
    "\n",
    "dataset = load_dataset(\"boolq\", split=\"validation\")\n",
    "\n",
    "def parse_answer(answer_text):\n",
    "    lines = answer_text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        cleaned = line.strip().strip(string.punctuation).lower()\n",
    "        if cleaned in ['yes', 'no']:\n",
    "            return cleaned == 'yes'\n",
    "    return None\n",
    "\n",
    "def get_ensemble_prediction(passage, question, true_answer):\n",
    "    prompt = f\"Passage: {passage}\\nQuestion: {question}\\nAnswer with yes or no.\"\n",
    "    model_funcs = [\n",
    "        (\"phi\", get_phi_completion),\n",
    "        (\"mistral\", get_mistral_completion),\n",
    "        (\"llama\", get_llama_completion),\n",
    "        (\"gpt4o_mini\", get_4o_mini_completion),\n",
    "        (\"gpt41_mini\", get_41_mini_completion)\n",
    "    ]\n",
    "    preds = []\n",
    "    model_preds = {}\n",
    "    for name, func in model_funcs:\n",
    "        try:\n",
    "            answer = func(prompt)\n",
    "            parsed = parse_answer(answer)\n",
    "            model_preds[f\"{name}_raw\"] = answer\n",
    "            model_preds[f\"{name}_pred\"] = parsed\n",
    "            if parsed is not None:\n",
    "                preds.append(parsed)\n",
    "        except Exception:\n",
    "            model_preds[f\"{name}_raw\"] = None\n",
    "            model_preds[f\"{name}_pred\"] = None\n",
    "    if len(preds) == 0:\n",
    "        return None, False, model_preds\n",
    "    vote = sum(preds) > len(preds) // 2\n",
    "    return vote, vote == true_answer, model_preds\n",
    "\n",
    "flagged_indices = set()\n",
    "results = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Evaluating Ensemble...\")\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    if i in flagged_indices:\n",
    "        continue\n",
    "\n",
    "    passage = example[\"passage\"]\n",
    "    question = example[\"question\"]\n",
    "    true_answer = example[\"answer\"]\n",
    "\n",
    "    pred, is_correct, model_preds = get_ensemble_prediction(passage, question, true_answer)\n",
    "\n",
    "    if pred is None:\n",
    "        flagged_indices.add(i)\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "        \"q_number\": i + 1,\n",
    "        \"passage\": passage,\n",
    "        \"question\": question,\n",
    "        \"actual_answer\": true_answer,\n",
    "        \"ensemble_pred\": pred,\n",
    "        \"phi_pred\": model_preds[\"phi_pred\"],\n",
    "        \"mistral_pred\": model_preds[\"mistral_pred\"],\n",
    "        \"llama_pred\": model_preds[\"llama_pred\"],\n",
    "        \"gpt4o_mini_pred\": model_preds[\"gpt4o_mini_pred\"],\n",
    "        \"gpt41_mini_pred\": model_preds[\"gpt41_mini_pred\"]\n",
    "    }\n",
    "    results.append(entry)\n",
    "\n",
    "    correct += int(is_correct)\n",
    "    total += 1\n",
    "\n",
    "\n",
    "acc_ensemble = correct / total if total > 0 else 0\n",
    "print(\"Evaluated Ensemble\")\n",
    "\n",
    "df_ensemble = pd.DataFrame(results)\n",
    "df_ensemble.to_csv(\"ensemble3_valid.csv\", index=False)\n",
    "print(\"Saved ensemble3_valid.csv\")\n",
    "print(f\"Ensemble Accuracy: {acc_ensemble:.2f}\")\n",
    "\n",
    "final_output = {\n",
    "    \"ensemble_accuracy\": acc_ensemble,\n",
    "    \"correct_count\": correct,\n",
    "    \"flagged_indices\": list(flagged_indices)\n",
    "}\n",
    "\n",
    "with open(\"ensemble3_valid.json\", \"w\") as f:\n",
    "    json.dump(final_output, f, indent=2)\n",
    "print(\"Saved ensemble3_valid.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
